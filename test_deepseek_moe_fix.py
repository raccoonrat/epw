#!/usr/bin/env python3
"""
æµ‹è¯•DeepSeek MoEæ¨¡å‹ä¿®å¤çš„è„šæœ¬
éªŒè¯é€‚é…å™¨æ¨¡å¼æ˜¯å¦è§£å†³äº†"Unrecognized configuration class"é”™è¯¯
"""

import os
import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å¯ç”¨å¿«é€ŸåŠ è½½
os.environ['EPW_FAST_LOADING'] = 'true'
os.environ['EPW_LOAD_IN_4BIT'] = 'true'  # ä½¿ç”¨4ä½é‡åŒ–ä»¥åŠ å¿«åŠ è½½

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("=== æµ‹è¯•DeepSeek MoEæ¨¡å‹åŠ è½½ ===")
    
    model_name = "deepseek-ai/deepseek-moe-16b-chat"
    print(f"æ¨¡å‹åç§°: {model_name}")
    
    # é…ç½®é‡åŒ–
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4"
    )
    
    try:
        # æµ‹è¯•1: åŠ è½½tokenizer
        print("\n1. åŠ è½½tokenizer...")
        tokenizer_start = time.time()
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        tokenizer_end = time.time()
        print(f"âœ“ TokenizeråŠ è½½æˆåŠŸï¼Œè€—æ—¶: {tokenizer_end - tokenizer_start:.2f}s")
        
        # æµ‹è¯•2: åŠ è½½åŸºç¡€æ¨¡å‹
        print("\n2. åŠ è½½åŸºç¡€æ¨¡å‹...")
        model_start = time.time()
        base_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        model_end = time.time()
        print(f"âœ“ åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè€—æ—¶: {model_end - model_start:.2f}s")
        print(f"  æ¨¡å‹ç±»å‹: {type(base_model).__name__}")
        
        # æµ‹è¯•3: åˆ›å»ºåŒ…è£…å™¨
        print("\n3. åˆ›å»ºæ¨¡å‹åŒ…è£…å™¨...")
        from epw_enhance_1 import MoEModelWithWatermark
        
        wrapper_start = time.time()
        model = MoEModelWithWatermark(base_model, tokenizer)
        wrapper_end = time.time()
        print(f"âœ“ æ¨¡å‹åŒ…è£…å™¨åˆ›å»ºæˆåŠŸï¼Œè€—æ—¶: {wrapper_end - wrapper_start:.2f}s")
        print(f"  ä¸“å®¶æ•°é‡: {model._num_experts}")
        print(f"  è·¯ç”±å™¨å“ˆå¸Œ: {model._router_hash[:16]}..." if model._router_hash else "æ— ")
        
        # æµ‹è¯•4: ç®€å•çš„å‰å‘ä¼ æ’­
        print("\n4. æµ‹è¯•å‰å‘ä¼ æ’­...")
        test_input = "Hello"
        inputs = tokenizer(test_input, return_tensors="pt")
        
        forward_start = time.time()
        with torch.no_grad():
            outputs = model.forward(**inputs, output_router_logits=True)
        forward_end = time.time()
        print(f"âœ“ å‰å‘ä¼ æ’­æˆåŠŸï¼Œè€—æ—¶: {forward_end - forward_start:.2f}s")
        
        if hasattr(outputs, 'router_logits'):
            print(f"  è·¯ç”±å™¨logitsæ•°é‡: {len(outputs.router_logits)}")
        else:
            print("  è­¦å‘Š: æœªæ‰¾åˆ°è·¯ç”±å™¨logits")
        
        # æµ‹è¯•5: æ–‡æœ¬ç”Ÿæˆ
        print("\n5. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
        generation_start = time.time()
        with torch.no_grad():
            generated = model.generate(
                **inputs,
                max_new_tokens=10,
                do_sample=True,
                temperature=0.7
            )
        generation_end = time.time()
        
        generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
        print(f"âœ“ æ–‡æœ¬ç”ŸæˆæˆåŠŸï¼Œè€—æ—¶: {generation_end - generation_start:.2f}s")
        print(f"  ç”Ÿæˆæ–‡æœ¬: {generated_text}")
        
        print("\n=== æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼DeepSeek MoEæ¨¡å‹ä¿®å¤æˆåŠŸ ===")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_watermark_functionality():
    """æµ‹è¯•æ°´å°åŠŸèƒ½"""
    print("\n=== æµ‹è¯•æ°´å°åŠŸèƒ½ ===")
    
    try:
        from epw_enhance_1 import EPWALogitsProcessor, WatermarkDetector
        
        # è¿™é‡Œéœ€è¦å…ˆåŠ è½½æ¨¡å‹ï¼Œä½†ä¸ºäº†ç®€åŒ–æµ‹è¯•ï¼Œæˆ‘ä»¬åªæµ‹è¯•ç±»å®šä¹‰
        print("âœ“ EPWALogitsProcessorç±»å®šä¹‰æ­£å¸¸")
        print("âœ“ WatermarkDetectorç±»å®šä¹‰æ­£å¸¸")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ°´å°åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("å¼€å§‹æµ‹è¯•DeepSeek MoEæ¨¡å‹ä¿®å¤...")
    
    # æµ‹è¯•æ¨¡å‹åŠ è½½
    loading_success = test_model_loading()
    
    # æµ‹è¯•æ°´å°åŠŸèƒ½
    watermark_success = test_watermark_functionality()
    
    if loading_success and watermark_success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é€‚é…å™¨æ¨¡å¼ä¿®å¤æˆåŠŸï¼")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•") 