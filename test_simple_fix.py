#!/usr/bin/env python3
"""
ç®€å•çš„DynamicCacheä¿®å¤æµ‹è¯•
"""

import os
import sys

def test_dynamic_cache_import():
    """æµ‹è¯•DynamicCacheå¯¼å…¥"""
    print("=== æµ‹è¯•DynamicCacheå¯¼å…¥ ===")
    
    try:
        import transformers
        print(f"transformersç‰ˆæœ¬: {transformers.__version__}")
        
        # å°è¯•ä¸åŒçš„å¯¼å…¥æ–¹æ³•
        DynamicCache = None
        
        # æ–¹æ³•1: ç›´æ¥ä»transformerså¯¼å…¥
        try:
            from transformers import DynamicCache
            print("âœ“ æ–¹æ³•1æˆåŠŸ: from transformers import DynamicCache")
        except ImportError as e:
            print(f"âœ— æ–¹æ³•1å¤±è´¥: {e}")
        
        # æ–¹æ³•2: ä»transformers.cacheå¯¼å…¥
        if DynamicCache is None:
            try:
                from transformers.cache import DynamicCache
                print("âœ“ æ–¹æ³•2æˆåŠŸ: from transformers.cache import DynamicCache")
            except ImportError as e:
                print(f"âœ— æ–¹æ³•2å¤±è´¥: {e}")
        
        # æ–¹æ³•3: ä»transformers.utilså¯¼å…¥
        if DynamicCache is None:
            try:
                from transformers.utils import DynamicCache
                print("âœ“ æ–¹æ³•3æˆåŠŸ: from transformers.utils import DynamicCache")
            except ImportError as e:
                print(f"âœ— æ–¹æ³•3å¤±è´¥: {e}")
        
        # æ–¹æ³•4: ä»transformers.generationå¯¼å…¥
        if DynamicCache is None:
            try:
                from transformers.generation import DynamicCache
                print("âœ“ æ–¹æ³•4æˆåŠŸ: from transformers.generation import DynamicCache")
            except ImportError as e:
                print(f"âœ— æ–¹æ³•4å¤±è´¥: {e}")
        
        # æ–¹æ³•5: åŠ¨æ€æŸ¥æ‰¾
        if DynamicCache is None:
            print("å°è¯•åŠ¨æ€æŸ¥æ‰¾DynamicCache...")
            for attr_name in dir(transformers):
                attr = getattr(transformers, attr_name)
                if hasattr(attr, '__name__') and attr.__name__ == 'DynamicCache':
                    DynamicCache = attr
                    print(f"âœ“ åŠ¨æ€æ‰¾åˆ°DynamicCache: {attr_name}")
                    break
        
        if DynamicCache is not None:
            print(f"âœ“ DynamicCacheç±»æ‰¾åˆ°: {DynamicCache}")
            
            # æ£€æŸ¥æ–¹æ³•
            if hasattr(DynamicCache, 'get_usable_length'):
                print("âœ“ DynamicCacheå·²æœ‰get_usable_lengthæ–¹æ³•")
            else:
                print("âš ï¸ DynamicCacheç¼ºå°‘get_usable_lengthæ–¹æ³•")
                
            if hasattr(DynamicCache, 'get_seq_length'):
                print("âœ“ DynamicCacheå·²æœ‰get_seq_lengthæ–¹æ³•")
            else:
                print("âš ï¸ DynamicCacheç¼ºå°‘get_seq_lengthæ–¹æ³•")
                
            return True
        else:
            print("âŒ æ— æ³•æ‰¾åˆ°DynamicCacheç±»")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_simple_model():
    """æµ‹è¯•ç®€å•æ¨¡å‹åŠ è½½"""
    print("\n=== æµ‹è¯•ç®€å•æ¨¡å‹åŠ è½½ ===")
    
    try:
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['EPW_FAST_LOADING'] = 'true'
        os.environ['EPW_LOAD_IN_4BIT'] = 'true'
        
        model_name = "deepseek-ai/deepseek-moe-16b-chat"
        print(f"æµ‹è¯•æ¨¡å‹: {model_name}")
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        print("âœ“ TokenizeråŠ è½½æˆåŠŸ")
        
        # é…ç½®é‡åŒ–
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•ç®€å•çš„å‰å‘ä¼ æ’­
        test_input = "Hello"
        inputs = tokenizer(test_input, return_tensors="pt")
        
        # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        model_device = next(model.parameters()).device
        inputs = {k: v.to(model_device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs, output_router_logits=True)
        
        print("âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
        
        # æµ‹è¯•ç”Ÿæˆ
        with torch.no_grad():
            generated = model.generate(
                **inputs,
                max_new_tokens=3,
                do_sample=True,
                temperature=0.7
            )
        
        generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
        print(f"âœ“ ç”ŸæˆæˆåŠŸ: {generated_text}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("å¼€å§‹ç®€å•æµ‹è¯•...")
    
    # æµ‹è¯•DynamicCacheå¯¼å…¥
    import_success = test_dynamic_cache_import()
    
    if import_success:
        print("\nâœ… DynamicCacheå¯¼å…¥æµ‹è¯•æˆåŠŸï¼")
        
        # æµ‹è¯•æ¨¡å‹åŠ è½½
        model_success = test_simple_model()
        
        if model_success:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
            print("ç°åœ¨å¯ä»¥è¿è¡Œä¸»ç¨‹åº: python epw-enhance-1.py")
        else:
            print("\nâš ï¸ æ¨¡å‹æµ‹è¯•å¤±è´¥ï¼Œä½†DynamicCacheå¯¼å…¥æˆåŠŸã€‚")
    else:
        print("\nâŒ DynamicCacheå¯¼å…¥æµ‹è¯•å¤±è´¥ã€‚") 