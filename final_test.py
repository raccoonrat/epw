#!/usr/bin/env python3
"""
æœ€ç»ˆæµ‹è¯•è„šæœ¬
éªŒè¯æ‰€æœ‰ä¿®å¤æ˜¯å¦æœ‰æ•ˆ
"""

import os
import sys

def test_all_fixes():
    """æµ‹è¯•æ‰€æœ‰ä¿®å¤"""
    print("=== æµ‹è¯•æ‰€æœ‰ä¿®å¤ ===")
    
    try:
        # 1. æµ‹è¯•DynamicCacheä¿®å¤
        print("1. æµ‹è¯•DynamicCacheä¿®å¤...")
        
        # å¯¼å…¥ä¸»ç¨‹åºä¸­çš„ä¿®å¤å‡½æ•°
        from epw_enhance_1 import apply_dynamic_cache_monkey_patch, fix_dynamic_cache_compatibility
        
        # åº”ç”¨ä¿®å¤
        apply_dynamic_cache_monkey_patch()
        fix_dynamic_cache_compatibility()
        
        # éªŒè¯ä¿®å¤
        try:
            import transformers
            
            # å°è¯•æ‰¾åˆ°DynamicCache
            DynamicCache = None
            for import_path in ['transformers', 'transformers.cache', 'transformers.utils', 'transformers.generation']:
                try:
                    module = __import__(import_path, fromlist=['DynamicCache'])
                    if hasattr(module, 'DynamicCache'):
                        DynamicCache = module.DynamicCache
                        break
                except:
                    continue
            
            if DynamicCache is not None:
                # æµ‹è¯•get_usable_lengthæ–¹æ³•
                cache = DynamicCache()
                if hasattr(cache, 'get_usable_length'):
                    result = cache.get_usable_length()
                    print(f"âœ“ get_usable_lengthæ–¹æ³•å¯ç”¨ï¼Œè¿”å›: {result}")
                else:
                    print("âŒ get_usable_lengthæ–¹æ³•ä¸å¯ç”¨")
                    return False
            else:
                print("âš ï¸ æ— æ³•æ‰¾åˆ°DynamicCacheç±»")
                
        except Exception as e:
            print(f"âŒ DynamicCacheæµ‹è¯•å¤±è´¥: {e}")
            return False
        
        # 2. æµ‹è¯•æ¨¡å‹åŠ è½½
        print("\n2. æµ‹è¯•æ¨¡å‹åŠ è½½...")
        
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['EPW_FAST_LOADING'] = 'true'
        os.environ['EPW_LOAD_IN_4BIT'] = 'true'
        
        model_name = "deepseek-ai/deepseek-moe-16b-chat"
        print(f"æµ‹è¯•æ¨¡å‹: {model_name}")
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        print("âœ“ TokenizeråŠ è½½æˆåŠŸ")
        
        # é…ç½®é‡åŒ–
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # 3. æµ‹è¯•å‰å‘ä¼ æ’­
        print("\n3. æµ‹è¯•å‰å‘ä¼ æ’­...")
        
        test_input = "Hello"
        inputs = tokenizer(test_input, return_tensors="pt")
        
        # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        model_device = next(model.parameters()).device
        inputs = {k: v.to(model_device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs, output_router_logits=True)
        
        print("âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
        
        # 4. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
        print("\n4. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
        
        with torch.no_grad():
            generated = model.generate(
                **inputs,
                max_new_tokens=5,
                do_sample=True,
                temperature=0.7
            )
        
        generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
        print(f"âœ“ ç”ŸæˆæˆåŠŸ: {generated_text}")
        
        # 5. æµ‹è¯•åŒ…è£…å™¨
        print("\n5. æµ‹è¯•æ¨¡å‹åŒ…è£…å™¨...")
        
        from epw_enhance_1 import MoEModelWithWatermark
        
        wrapped_model = MoEModelWithWatermark(model, tokenizer)
        print("âœ“ æ¨¡å‹åŒ…è£…å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•åŒ…è£…å™¨çš„ç”Ÿæˆ
        with torch.no_grad():
            wrapped_generated = wrapped_model.generate(
                **inputs,
                max_new_tokens=3,
                do_sample=True,
                temperature=0.7
            )
        
        wrapped_text = tokenizer.decode(wrapped_generated[0], skip_special_tokens=True)
        print(f"âœ“ åŒ…è£…å™¨ç”ŸæˆæˆåŠŸ: {wrapped_text}")
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("å¼€å§‹æœ€ç»ˆæµ‹è¯•...")
    success = test_all_fixes()
    
    if success:
        print("\nâœ… æ‰€æœ‰ä¿®å¤æˆåŠŸï¼ç°åœ¨å¯ä»¥è¿è¡Œä¸»ç¨‹åºäº†ã€‚")
        print("è¿è¡Œå‘½ä»¤: python epw-enhance-1.py")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚") 