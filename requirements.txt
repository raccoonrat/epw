# EPW-A Framework Requirements
# Expert Pathway Watermarking (EPW-A) for Mixture-of-Experts (MoE) LLMs

# Core ML/AI Framework
torch>=2.0.0
transformers>=4.35.0

# Hugging Face Hub for model loading
huggingface-hub>=0.19.0

# Quantization support (optional but recommended)
bitsandbytes>=0.41.0

# Scientific computing for PEPI oracle
numpy>=1.24.0
scikit-learn>=1.3.0

# Additional utilities
accelerate>=0.24.0
safetensors>=0.4.0

# Optional: For better performance on specific hardware
# xformers>=0.0.22  # Uncomment for better attention performance
# flash-attn>=2.3.0  # Uncomment for flash attention support

# Development and testing (optional)
# pytest>=7.4.0
# black>=23.0.0
# flake8>=6.0.0 