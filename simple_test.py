#!/usr/bin/env python3
"""
ç®€å•çš„DeepSeek MoEæ¨¡å‹æµ‹è¯•è„šæœ¬
éªŒè¯DynamicCacheå…¼å®¹æ€§ä¿®å¤
"""

import os
import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['EPW_FAST_LOADING'] = 'true'
os.environ['EPW_LOAD_IN_4BIT'] = 'true'

def test_simple_generation():
    """ç®€å•çš„æ–‡æœ¬ç”Ÿæˆæµ‹è¯•"""
    print("=== ç®€å•æ–‡æœ¬ç”Ÿæˆæµ‹è¯• ===")
    
    model_name = "deepseek-ai/deepseek-moe-16b-chat"
    print(f"æ¨¡å‹: {model_name}")
    
    try:
        # 1. åŠ è½½tokenizer
        print("1. åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        print("âœ“ TokenizeråŠ è½½æˆåŠŸ")
        
        # 2. é…ç½®é‡åŒ–
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        # 3. åŠ è½½æ¨¡å‹
        print("2. åŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # 4. å‡†å¤‡è¾“å…¥
        test_prompt = "Hello"
        inputs = tokenizer(test_prompt, return_tensors="pt")
        
        # 5. ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        model_device = next(model.parameters()).device
        print(f"æ¨¡å‹è®¾å¤‡: {model_device}")
        inputs = {k: v.to(model_device) for k, v in inputs.items()}
        
        # 6. ç”Ÿæˆæ–‡æœ¬
        print("3. ç”Ÿæˆæ–‡æœ¬...")
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=10,
                do_sample=True,
                temperature=0.7
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"âœ“ ç”ŸæˆæˆåŠŸ: {generated_text}")
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("å¼€å§‹ç®€å•æµ‹è¯•...")
    success = test_simple_generation()
    
    if success:
        print("\nâœ… ä¿®å¤æˆåŠŸï¼ç°åœ¨å¯ä»¥è¿è¡Œä¸»ç¨‹åºäº†ã€‚")
    else:
        print("\nâŒ ä¿®å¤å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚") 