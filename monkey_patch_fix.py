#!/usr/bin/env python3
"""
çŒ´å­è¡¥ä¸ä¿®å¤DynamicCacheé—®é¢˜
"""

import os
import sys
import types

def apply_monkey_patch():
    """åº”ç”¨çŒ´å­è¡¥ä¸ä¿®å¤"""
    print("=== åº”ç”¨çŒ´å­è¡¥ä¸ä¿®å¤ ===")
    
    try:
        import transformers
        
        # æ–¹æ³•1: ç›´æ¥ä¿®æ”¹transformers.generation.utils
        try:
            from transformers.generation import utils as generation_utils
            
            # æ£€æŸ¥æ˜¯å¦æœ‰_sampleæ–¹æ³•
            if hasattr(generation_utils, '_sample'):
                original_sample = generation_utils._sample
                
                def patched_sample(self, *args, **kwargs):
                    """ä¿®è¡¥çš„_sampleæ–¹æ³•"""
                    try:
                        return original_sample(self, *args, **kwargs)
                    except AttributeError as e:
                        if "'DynamicCache' object has no attribute 'get_usable_length'" in str(e):
                            # åŠ¨æ€æ·»åŠ get_usable_lengthæ–¹æ³•
                            import torch
                            from transformers.cache import DynamicCache
                            
                            if not hasattr(DynamicCache, 'get_usable_length'):
                                def get_usable_length(self):
                                    return self.get_seq_length()
                                DynamicCache.get_usable_length = get_usable_length
                                print("âœ“ åŠ¨æ€æ·»åŠ äº†get_usable_lengthæ–¹æ³•")
                            
                            # é‡è¯•
                            return original_sample(self, *args, **kwargs)
                        else:
                            raise e
                
                # æ›¿æ¢æ–¹æ³•
                generation_utils._sample = patched_sample
                print("âœ“ å·²ä¿®è¡¥_sampleæ–¹æ³•")
                
        except Exception as e:
            print(f"âš ï¸ ä¿®è¡¥_sampleæ–¹æ³•å¤±è´¥: {e}")
        
        # æ–¹æ³•2: ç›´æ¥ä¿®è¡¥DynamicCacheç±»
        try:
            # å°è¯•æ‰¾åˆ°DynamicCacheç±»
            DynamicCache = None
            
            # å°è¯•ä¸åŒçš„å¯¼å…¥è·¯å¾„
            for import_path in [
                'transformers',
                'transformers.cache',
                'transformers.utils',
                'transformers.generation'
            ]:
                try:
                    module = __import__(import_path, fromlist=['DynamicCache'])
                    if hasattr(module, 'DynamicCache'):
                        DynamicCache = module.DynamicCache
                        print(f"âœ“ ä»{import_path}æ‰¾åˆ°DynamicCache")
                        break
                except:
                    continue
            
            if DynamicCache is not None:
                # æ·»åŠ get_usable_lengthæ–¹æ³•
                if not hasattr(DynamicCache, 'get_usable_length'):
                    def get_usable_length(self):
                        """å…¼å®¹æ€§æ–¹æ³•"""
                        return self.get_seq_length()
                    
                    DynamicCache.get_usable_length = get_usable_length
                    print("âœ“ å·²ä¸ºDynamicCacheæ·»åŠ get_usable_lengthæ–¹æ³•")
                else:
                    print("âœ“ DynamicCacheå·²æœ‰get_usable_lengthæ–¹æ³•")
            else:
                print("âš ï¸ æ— æ³•æ‰¾åˆ°DynamicCacheç±»")
                
        except Exception as e:
            print(f"âš ï¸ ä¿®è¡¥DynamicCacheå¤±è´¥: {e}")
        
        # æ–¹æ³•3: ä¿®è¡¥transformersçš„å¯¼å…¥
        try:
            # åˆ›å»ºä¸€ä¸ªå…¼å®¹çš„DynamicCache
            class CompatibleDynamicCache:
                def __init__(self):
                    self._cache = {}
                
                def get_seq_length(self):
                    return 0
                
                def get_usable_length(self):
                    return self.get_seq_length()
            
            # å°è¯•æ›¿æ¢transformersä¸­çš„DynamicCache
            for module_name in ['transformers.cache', 'transformers.utils', 'transformers.generation']:
                try:
                    module = __import__(module_name, fromlist=['DynamicCache'])
                    if hasattr(module, 'DynamicCache'):
                        module.DynamicCache = CompatibleDynamicCache
                        print(f"âœ“ å·²æ›¿æ¢{module_name}ä¸­çš„DynamicCache")
                except:
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ æ›¿æ¢DynamicCacheå¤±è´¥: {e}")
        
        print("âœ“ çŒ´å­è¡¥ä¸ä¿®å¤å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ çŒ´å­è¡¥ä¸ä¿®å¤å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_model_after_patch():
    """æµ‹è¯•ä¿®è¡¥åçš„æ¨¡å‹"""
    print("\n=== æµ‹è¯•ä¿®è¡¥åçš„æ¨¡å‹ ===")
    
    try:
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['EPW_FAST_LOADING'] = 'true'
        os.environ['EPW_LOAD_IN_4BIT'] = 'true'
        
        model_name = "deepseek-ai/deepseek-moe-16b-chat"
        print(f"æµ‹è¯•æ¨¡å‹: {model_name}")
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        print("âœ“ TokenizeråŠ è½½æˆåŠŸ")
        
        # é…ç½®é‡åŒ–
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•ç”Ÿæˆ
        test_input = "Hello"
        inputs = tokenizer(test_input, return_tensors="pt")
        
        # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        model_device = next(model.parameters()).device
        inputs = {k: v.to(model_device) for k, v in inputs.items()}
        
        print("æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
        with torch.no_grad():
            generated = model.generate(
                **inputs,
                max_new_tokens=5,
                do_sample=True,
                temperature=0.7
            )
        
        generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
        print(f"âœ“ ç”ŸæˆæˆåŠŸ: {generated_text}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("å¼€å§‹çŒ´å­è¡¥ä¸ä¿®å¤...")
    
    # åº”ç”¨ä¿®å¤
    patch_success = apply_monkey_patch()
    
    if patch_success:
        print("\nâœ… çŒ´å­è¡¥ä¸ä¿®å¤æˆåŠŸï¼")
        
        # æµ‹è¯•æ¨¡å‹
        model_success = test_model_after_patch()
        
        if model_success:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
            print("ç°åœ¨å¯ä»¥è¿è¡Œä¸»ç¨‹åº: python epw-enhance-1.py")
        else:
            print("\nâš ï¸ æ¨¡å‹æµ‹è¯•å¤±è´¥ï¼Œä½†çŒ´å­è¡¥ä¸ä¿®å¤æˆåŠŸã€‚")
    else:
        print("\nâŒ çŒ´å­è¡¥ä¸ä¿®å¤å¤±è´¥ã€‚") 