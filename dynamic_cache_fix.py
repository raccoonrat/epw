#!/usr/bin/env python3
"""
DynamicCacheä¿®å¤è„šæœ¬
åœ¨è¿è¡Œä¸»ç¨‹åºä¹‹å‰å…ˆè¿è¡Œæ­¤è„šæœ¬
"""

import sys
import os

def apply_dynamic_cache_fix():
    """åº”ç”¨DynamicCacheä¿®å¤"""
    print("=== åº”ç”¨DynamicCacheä¿®å¤ ===")
    
    try:
        # å¯¼å…¥transformers
        import transformers
        
        # æ£€æŸ¥æ˜¯å¦æœ‰cacheæ¨¡å—
        if hasattr(transformers, 'cache'):
            from transformers.cache import DynamicCache
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ get_usable_lengthæ–¹æ³•
            if not hasattr(DynamicCache, 'get_usable_length'):
                def get_usable_length(self):
                    """å…¼å®¹æ€§æ–¹æ³•ï¼Œè¿”å›åºåˆ—é•¿åº¦"""
                    return self.get_seq_length()
                
                # åŠ¨æ€æ·»åŠ æ–¹æ³•
                DynamicCache.get_usable_length = get_usable_length
                print("âœ“ å·²ä¸ºDynamicCacheæ·»åŠ get_usable_lengthæ–¹æ³•")
            else:
                print("âœ“ DynamicCacheå·²åŒ…å«get_usable_lengthæ–¹æ³•")
            
            # æµ‹è¯•ä¿®å¤æ˜¯å¦æœ‰æ•ˆ
            cache = DynamicCache()
            try:
                result = cache.get_usable_length()
                print(f"âœ“ æµ‹è¯•æˆåŠŸï¼Œget_usable_lengthè¿”å›: {result}")
                return True
            except Exception as e:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
                return False
                
        else:
            print("âš ï¸ transformersæ²¡æœ‰cacheæ¨¡å—")
            return False
            
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥transformers: {e}")
        return False
    except Exception as e:
        print(f"âŒ ä¿®å¤å¤±è´¥: {e}")
        return False

def test_model_generation():
    """æµ‹è¯•æ¨¡å‹ç”ŸæˆåŠŸèƒ½"""
    print("\n=== æµ‹è¯•æ¨¡å‹ç”ŸæˆåŠŸèƒ½ ===")
    
    try:
        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['EPW_FAST_LOADING'] = 'true'
        os.environ['EPW_LOAD_IN_4BIT'] = 'true'
        
        model_name = "deepseek-ai/deepseek-moe-16b-chat"
        print(f"æµ‹è¯•æ¨¡å‹: {model_name}")
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        print("âœ“ TokenizeråŠ è½½æˆåŠŸ")
        
        # é…ç½®é‡åŒ–
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•ç”Ÿæˆ
        test_input = "Hello"
        inputs = tokenizer(test_input, return_tensors="pt")
        
        # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        model_device = next(model.parameters()).device
        inputs = {k: v.to(model_device) for k, v in inputs.items()}
        
        print("æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
        with torch.no_grad():
            generated = model.generate(
                **inputs,
                max_new_tokens=5,
                do_sample=True,
                temperature=0.7
            )
        
        generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
        print(f"âœ“ ç”ŸæˆæˆåŠŸ: {generated_text}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("å¼€å§‹DynamicCacheä¿®å¤...")
    
    # åº”ç”¨ä¿®å¤
    fix_success = apply_dynamic_cache_fix()
    
    if fix_success:
        print("\nâœ… DynamicCacheä¿®å¤æˆåŠŸï¼")
        
        # æµ‹è¯•æ¨¡å‹ç”Ÿæˆ
        model_success = test_model_generation()
        
        if model_success:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç°åœ¨å¯ä»¥è¿è¡Œä¸»ç¨‹åºäº†ã€‚")
            print("è¿è¡Œå‘½ä»¤: python epw-enhance-1.py")
        else:
            print("\nâš ï¸ æ¨¡å‹æµ‹è¯•å¤±è´¥ï¼Œä½†DynamicCacheä¿®å¤æˆåŠŸã€‚")
    else:
        print("\nâŒ DynamicCacheä¿®å¤å¤±è´¥ã€‚") 