import torch
import hashlib
import random
import numpy as np
import pickle
from transformers import LogitsProcessor
from typing import List, Tuple, Dict, Any
from sklearn.linear_model import LogisticRegression
from scipy.stats import norm

# --- æ ¸å¿ƒå·¥å…·å‡½æ•° ---

def get_router_hash(model: torch.nn.Module, moe_layer_name: str = "block_sparse_moe") -> str:
    """
    è®¡ç®—å¹¶è¿”å›æ¨¡å‹è·¯ç”±å™¨æƒé‡çš„SHA256å“ˆå¸Œå€¼ (IRSHå®ç°)ã€‚
    ä¸“é—¨é’ˆå¯¹Mixtralæ¨¡å‹æ¶æ„ä¼˜åŒ–ã€‚
    """
    try:
        # æ£€æŸ¥æ¨¡å‹ç±»å‹
        model_type = type(model).__name__
        print(f"æ£€æµ‹åˆ°æ¨¡å‹ç±»å‹: {model_type}")
        
        # å¯¹äºMixtralæ¨¡å‹ï¼Œéå†æ‰€æœ‰å±‚æŸ¥æ‰¾MoEå±‚
        if hasattr(model, 'model') and hasattr(model.model, 'layers'):
            router_weights = []
            moe_layer_count = 0
            
            for i, layer in enumerate(model.model.layers):
                if hasattr(layer, 'block_sparse_moe') and hasattr(layer.block_sparse_moe, 'gate'):
                    try:
                        gate_weights = layer.block_sparse_moe.gate.weight.data
                        
                        # æ£€æŸ¥æ˜¯å¦ä¸ºmeta tensor
                        if hasattr(gate_weights, 'is_meta') and gate_weights.is_meta:
                            print(f"è­¦å‘Š: ç¬¬{i}å±‚çš„è·¯ç”±å™¨æƒé‡æ˜¯meta tensor")
                            continue
                        
                        # å®‰å…¨è®¿é—®æ•°æ®
                        weight_bytes = gate_weights.cpu().numpy().tobytes()
                        router_weights.append(weight_bytes)
                        moe_layer_count += 1
                        
                    except Exception as e:
                        print(f"è­¦å‘Š: æ— æ³•è®¿é—®ç¬¬{i}å±‚çš„è·¯ç”±å™¨æƒé‡: {e}")
                        continue
            
            if router_weights:
                print(f"æ‰¾åˆ° {moe_layer_count} ä¸ªMoEå±‚")
                # è¿æ¥æ‰€æœ‰è·¯ç”±å™¨æƒé‡å¹¶å“ˆå¸Œ
                combined_weights = b''.join(router_weights)
                hasher = hashlib.sha256()
                hasher.update(combined_weights)
                return hasher.hexdigest()
            else:
                print("æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„MoEå±‚")
        
        # å°è¯•å…¶ä»–å¯èƒ½çš„æ¶æ„
        alternative_paths = [
            'model.block_sparse_moe.gate.weight',
            'model.moe.gate.weight',
            'block_sparse_moe.gate.weight',
            'moe.gate.weight'
        ]
        
        for path in alternative_paths:
            try:
                # ä½¿ç”¨getattré€’å½’è®¿é—®
                parts = path.split('.')
                current = model
                for part in parts:
                    current = getattr(current, part)
                
                weight_data = current.data
                hasher = hashlib.sha256()
                hasher.update(weight_data.cpu().numpy().tobytes())
                print(f"ä½¿ç”¨è·¯å¾„ '{path}' æˆåŠŸè®¡ç®—å“ˆå¸Œ")
                return hasher.hexdigest()
            except (AttributeError, Exception) as e:
                continue
        
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å“ˆå¸Œ
        print("æ— æ³•æ‰¾åˆ°è·¯ç”±å™¨æƒé‡ï¼Œä½¿ç”¨é»˜è®¤å“ˆå¸Œ")
        return "default_router_hash_for_mixtral"
        
    except Exception as e:
        print(f"è·¯ç”±å™¨å“ˆå¸Œè®¡ç®—è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return "error_router_hash"

def get_green_list_ids(
    key: str,
    expert_index: int,
    router_hash: str,
    vocab_size: int,
    gamma: float = 0.5
) -> List[int]:
    """
    æ ¹æ®å¯†é’¥ã€ä¸“å®¶ç´¢å¼•å’Œè·¯ç”±å™¨å“ˆå¸Œç”Ÿæˆç»¿åå• (IRSHå®ç°)ã€‚
    """
    combined_input = f"{key}-{expert_index}-{router_hash}"
    hasher = hashlib.sha256()
    hasher.update(combined_input.encode('utf-8'))
    seed = int.from_bytes(hasher.digest(), 'big')
    
    rng = random.Random(seed)
    green_list_size = int(vocab_size * gamma)
    green_list = rng.sample(range(vocab_size), green_list_size)
    return green_list

# --- æ¨¡å‹åŒ…è£…å™¨ä¸æŠ½è±¡ ---

class MoEModelWrapper:
    """
    ä¸€ä¸ªæŠ½è±¡åŒ…è£…å™¨ï¼Œç”¨äºè§£è€¦æ°´å°é€»è¾‘ä¸å…·ä½“æ¨¡å‹å®ç°ã€‚
    """
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def get_router_weights(self):
        # å®é™…å®ç°éœ€è¦æ ¹æ®æ¨¡å‹æ¶æ„è°ƒæ•´
        try:
            # å°è¯•ç›´æ¥è®¿é—®
            return self.model.model.block_sparse_moe.gate.weight
        except AttributeError:
            # å¦‚æœç›´æ¥è®¿é—®å¤±è´¥ï¼Œå°è¯•éå†å±‚
            for layer in self.model.model.layers:
                if hasattr(layer, 'block_sparse_moe') and hasattr(layer.block_sparse_moe, 'gate'):
                    return layer.block_sparse_moe.gate.weight
            # å¦‚æœéƒ½å¤±è´¥ï¼Œè¿”å›None
            return None

    def get_vocab_size(self) -> int:
        return self.model.config.vocab_size

    def get_logits_and_route_info(self, input_ids: torch.Tensor) -> Tuple:
        """
        æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œè¿”å›logitsã€top-1ä¸“å®¶ç´¢å¼•å’Œå…¶ç½®ä¿¡åº¦ã€‚
        è¿™æ˜¯ç°ç›’è®¿é—®çš„æ ¸å¿ƒã€‚
        """
        # ç¡®ä¿è¾“å…¥å¼ é‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if hasattr(self.model, 'device'):
            device = self.model.device
        else:
            device = next(self.model.parameters()).device
        
        input_ids = input_ids.to(device)
        
        with torch.no_grad():
            outputs = self.model(input_ids, output_router_logits=True)
            
        logits = outputs.logits[:, -1, :]
        
        # å¤„ç†router_logitsï¼ˆæ”¯æŒæ¨¡æ‹Ÿæ¨¡å¼ï¼‰
        if hasattr(outputs, 'router_logits') and outputs.router_logits:
            router_logits = outputs.router_logits[-1]
            if router_logits.dim() == 3:
                router_logits = router_logits[0, -1, :]
            else:
                router_logits = router_logits[0, :]
            
            probs = torch.softmax(router_logits, dim=-1)
            top_expert_confidence, top_expert_index = torch.max(probs, dim=-1)
        else:
            # æ¨¡æ‹Ÿæ¨¡å¼ï¼šéšæœºé€‰æ‹©ä¸“å®¶
            top_expert_index = torch.randint(0, 8, (1,)).item()
            top_expert_confidence = torch.rand(1).item()
        
        return logits, top_expert_index, top_expert_confidence

    def get_logits_blackbox(self, input_ids: torch.Tensor) -> torch.Tensor:
        """
        æ¨¡æ‹Ÿé»‘ç›’APIï¼Œåªè¿”å›logitsã€‚
        """
        # ç¡®ä¿è¾“å…¥å¼ é‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if hasattr(self.model, 'device'):
            device = self.model.device
        else:
            device = next(self.model.parameters()).device
        
        input_ids = input_ids.to(device)
        
        with torch.no_grad():
            outputs = self.model(input_ids)
        return outputs.logits[:, -1, :]

# --- æ°´å°ç”Ÿæˆå™¨ ---

class EPW_A_Watermarker(LogitsProcessor):
    """
    å®ç°EPW-Aç”Ÿæˆç®—æ³•çš„LogitsProcessorã€‚
    """
    def __init__(self,
                 model_wrapper: MoEModelWrapper,
                 secret_key: str,
                 gamma: float = 0.5,
                 delta_config: Dict[str, Any] = None):
        
        self.wrapper = model_wrapper
        self.secret_key = secret_key
        self.gamma = gamma
        self.vocab_size = self.wrapper.get_vocab_size()
        
        # IRSH: åœ¨åˆå§‹åŒ–æ—¶è®¡ç®—å¹¶å­˜å‚¨è·¯ç”±å™¨å“ˆå¸Œ
        self.router_hash = get_router_hash(self.wrapper.model)
        
        # EWP / GSG é…ç½®
        self.is_ewp = delta_config.get("is_ewp", False) if delta_config else False
        if self.is_ewp:
            self.base_deltas = delta_config["base_deltas"] # e.g., [2.0, 2.1,...]
        else:
            self.global_delta = delta_config.get("global_delta", 2.0) if delta_config else 2.0

    def __call__(self, input_ids: torch.Tensor, scores: torch.Tensor) -> torch.Tensor:
        # è·å–è·¯ç”±ä¿¡æ¯ï¼ˆéœ€è¦ç°ç›’è®¿é—®ï¼‰
        _, top_expert_index, confidence = self.wrapper.get_logits_and_route_info(input_ids)

        # è®¡ç®—ç§å­ (èå…¥IRSH)
        # æ³¨æ„ï¼šè¿™é‡Œçš„get_green_list_idså·²ç»å®ç°äº†PRF
        green_list = get_green_list_ids(
            self.secret_key,
            top_expert_index,
            self.router_hash,
            self.vocab_size,
            self.gamma
        )
        
        # è®¡ç®—æ°´å°å¼ºåº¦ (æ”¯æŒEWP)
        if self.is_ewp:
            base_delta = self.base_deltas[top_expert_index]
            # å¢åŠ ä¸€ä¸ªepsiloné˜²æ­¢é™¤ä»¥é›¶
            effective_delta = base_delta / (confidence + 1e-8)
        else: # GSG
            effective_delta = self.global_delta
            
        # ä¿®æ”¹ Logits
        scores[:, green_list] += effective_delta
        return scores

# --- æ°´å°æ£€æµ‹å™¨ ---

class EPW_A_Detector:
    """
    å®ç°EPW-Aæ£€æµ‹å¥—ä»¶ï¼ŒåŒ…æ‹¬ç°ç›’(CSPV)å’Œé»‘ç›’(PEPI)æ£€æµ‹ã€‚
    """
    def __init__(self,
                 model_wrapper: MoEModelWrapper,
                 secret_key: str,
                 router_hash: str,
                 gamma: float = 0.5):
        
        self.wrapper = model_wrapper
        self.secret_key = secret_key
        self.router_hash = router_hash # æ£€æµ‹æ—¶éœ€è¦åŸå§‹çš„router_hash
        self.gamma = gamma
        self.vocab_size = self.wrapper.get_vocab_size()

    def _calculate_z_score(self, green_tokens: int, total_tokens: int) -> float:
        if total_tokens == 0:
            return 0.0
        expected_green = total_tokens * self.gamma
        std_dev = np.sqrt(total_tokens * self.gamma * (1 - self.gamma))
        return (green_tokens - expected_green) / (std_dev + 1e-8)

    def detect_graybox_cspv(self, text: str, sample_size: int = 50) -> float:
        """
        ä½¿ç”¨ç½®ä¿¡åº¦åˆ†å±‚è·¯å¾„éªŒè¯ (CSPV) è¿›è¡Œé«˜æ•ˆç°ç›’æ£€æµ‹ã€‚
        """
        token_ids = self.wrapper.tokenizer.encode(text, return_tensors='pt')
        if token_ids.shape[1] <= 1: return 0.0

        # 1. å•æ¬¡å‰å‘ä¼ æ’­è·å–æ‰€æœ‰è·¯ç”±ç½®ä¿¡åº¦
        all_confidences = []
        for t in range(1, token_ids.shape[1]):
            context = token_ids[:, :t]
            _, _, confidence = self.wrapper.get_logits_and_route_info(context)
            all_confidences.append((t, confidence))

        # 2. ç½®ä¿¡åº¦åˆ†å±‚æŠ½æ · (CSPV)
        if len(all_confidences) <= sample_size:
            sampled_indices = [item for item in all_confidences]
        else:
            all_confidences.sort(key=lambda x: x)
            k = sample_size // 3
            low_conf = [item for item in all_confidences[:k]]
            high_conf = [item for item in all_confidences[-k:]]
            remaining = [item for item in all_confidences[k:-k]]
            random_conf = random.sample(remaining, sample_size - 2 * k)
            sampled_indices = low_conf + high_conf + random_conf
        
        # 3. å¯¹æŠ½æ ·ç‚¹è¿›è¡Œé€ä¸€éªŒè¯
        green_token_count = 0
        for t, confidence in sampled_indices:
            context = token_ids[:, :t]
            _, top_expert_index, _ = self.wrapper.get_logits_and_route_info(context)
            
            green_list = get_green_list_ids(
                self.secret_key, top_expert_index, self.router_hash, self.vocab_size, self.gamma
            )
            
            if token_ids[0, t].item() in green_list:
                green_token_count += 1
        
        # 4. è®¡ç®— Z-score
        return self._calculate_z_score(green_token_count, len(sampled_indices))

    def detect_blackbox_pepi(self, text: str, oracle) -> float:
        """
        ä½¿ç”¨æ¦‚ç‡æ€§ä¸“å®¶è·¯å¾„æ¨æ–­ (PEPI) è¿›è¡Œé»‘ç›’æ£€æµ‹ã€‚
        """
        token_ids = self.wrapper.tokenizer.encode(text, return_tensors='pt')
        if token_ids.shape[1] <= 1: return 0.0

        green_token_count = 0
        num_tokens = token_ids.shape[1] - 1

        for t in range(1, token_ids.shape[1]):
            context = token_ids[:, :t]
            # a. ä»é»‘ç›’ API è·å– Logits
            logits = self.wrapper.get_logits_blackbox(context)
            
            # b. ä½¿ç”¨é¢„è¨€æœºæ¨æ–­ä¸“å®¶è·¯å¾„ (PEPI)
            predicted_expert_index = oracle.predict(logits.cpu().numpy())
            
            # c. é‡å»ºç»¿åå• (èå…¥IRSH)
            green_list = get_green_list_ids(
                self.secret_key, predicted_expert_index, self.router_hash, self.vocab_size, self.gamma
            )
            
            # d. ç»Ÿè®¡å‘½ä¸­
            if token_ids[0, t].item() in green_list:
                green_token_count += 1
        
        # 4. è®¡ç®— Z-score
        return self._calculate_z_score(green_token_count, num_tokens)

# --- PEPI é¢„è¨€æœºè®­ç»ƒ ---

def train_pepi_oracle(model_wrapper: MoEModelWrapper, training_corpus: List[str], model_path: str = "pepi_oracle.pkl"):
    """
    è®­ç»ƒå¹¶ä¿å­˜PEPIè·¯å¾„æ¨æ–­é¢„è¨€æœºã€‚
    """
    X_logits, Y_experts = [], []
    print("æ­£åœ¨ç”ŸæˆPEPIè®­ç»ƒæ•°æ®...")
    for text in training_corpus:
        token_ids = model_wrapper.tokenizer.encode(text, return_tensors='pt')
        for t in range(1, token_ids.shape[1]):
            context = token_ids[:, :t]
            logits, expert_index, _ = model_wrapper.get_logits_and_route_info(context)
            X_logits.append(logits.cpu().numpy().flatten())
            Y_experts.append(expert_index)
    
    print(f"æ•°æ®ç”Ÿæˆå®Œæ¯•ï¼Œå…± {len(Y_experts)} ä¸ªæ ·æœ¬ã€‚å¼€å§‹è®­ç»ƒåˆ†ç±»å™¨...")
    oracle = LogisticRegression(max_iter=1000, solver='liblinear')
    oracle.fit(X_logits, Y_experts)
    
    print("è®­ç»ƒå®Œæˆã€‚æ­£åœ¨ä¿å­˜æ¨¡å‹...")
    with open(model_path, 'wb') as f:
        pickle.dump(oracle, f)
    
    print(f"PEPIé¢„è¨€æœºå·²ä¿å­˜è‡³ {model_path}")
    return oracle

def load_pepi_oracle(model_path: str = "pepi_oracle.pkl"):
    """åŠ è½½å·²è®­ç»ƒçš„PEPIé¢„è¨€æœºã€‚"""
    with open(model_path, 'rb') as f:
        oracle = pickle.load(f)
    return oracle

# --- å®éªŒä¸€ï¼šç”Ÿæˆå¹¶æ£€æµ‹å¸¦æ°´å°çš„æ–‡æœ¬ ---

if __name__ == "__main__":
    print("=== EPW-A å¢å¼ºç‰ˆå®éªŒä¸€ï¼šç”Ÿæˆå¹¶æ£€æµ‹å¸¦æ°´å°çš„æ–‡æœ¬ ===")
    
    # 1. ç¯å¢ƒè®¾ç½®å’Œæ¨¡å‹åŠ è½½
    print("\n1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
    
    # 4ä½é‡åŒ–é…ç½®
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype="float16"
    )
    
    # è®¾ç½®æ¨¡å‹è·¯å¾„ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
    model_paths = [
        # æœ¬åœ°è·¯å¾„ï¼ˆå¦‚æœéœ€è¦ï¼‰
        # "/path/to/your/local/model",
        "/root/private_data/model/mixtral-8x7b", 
        "/work/home/scnttrxbp8/wangyh/Mixtral-8x7B-Instruct-v0.1",
        "microsoft/DialoGPT-small",  # å°å‹æ¨¡å‹ï¼Œé€‚åˆæµ‹è¯•
        "gpt2",  # æ ‡å‡†GPT-2æ¨¡å‹
        "microsoft/DialoGPT-medium",  # ä¸­å‹æ¨¡å‹
    ]
    
    model_id = None
    tokenizer = None
    model = None
    
    for path in model_paths:
        try:
            # å°è¯•åŠ è½½æ¨¡å‹
            tokenizer = AutoTokenizer.from_pretrained(path)
            model = AutoModelForCausalLM.from_pretrained(
                path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                quantization_config=quantization_config,
            )
            model_id = path
            print(f"âœ“ æˆåŠŸåŠ è½½æ¨¡å‹: {path}")
            break
        except Exception as e:
            print(f"âœ— æ— æ³•åŠ è½½æ¨¡å‹ {path}: {e}")
            continue
    
    if model_id is None:
        print("âœ— æ‰€æœ‰æ¨¡å‹è·¯å¾„éƒ½æ— æ³•åŠ è½½ï¼Œåˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼...")
        print("æ³¨æ„ï¼šæ¨¡æ‹Ÿæ¨¡å¼å°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•ï¼Œä¸ä¼šè¿›è¡Œå®é™…çš„æ¨¡å‹æ¨ç†")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹å’Œåˆ†è¯å™¨
        class MockTokenizer:
            def __init__(self):
                self.vocab_size = 50257  # GPT-2è¯æ±‡è¡¨å¤§å°
                self.pad_token = "<|endoftext|>"
                self.eos_token = "<|endoftext|>"
            
            def encode(self, text, return_tensors='pt', add_special_tokens=False):
                # ç®€å•çš„æ¨¡æ‹Ÿç¼–ç 
                tokens = [hash(text) % self.vocab_size] + [i % self.vocab_size for i in range(len(text.split()))]
                return torch.tensor([tokens])
            
            def decode(self, token_ids, skip_special_tokens=True):
                # ç¡®ä¿token_idsæ˜¯å¼ é‡æ ¼å¼
                if isinstance(token_ids, torch.Tensor):
                    if token_ids.dim() == 1:
                        token_list = token_ids.tolist()
                    else:
                        token_list = token_ids[0].tolist()
                else:
                    token_list = token_ids
                return "æ¨¡æ‹Ÿç”Ÿæˆçš„æ–‡æœ¬: " + " ".join([f"token_{i}" for i in token_list])
        
        class MockModel:
            def __init__(self):
                self.config = type('obj', (object,), {'vocab_size': 50257})()
                self.device = 'cpu'
            
            def __call__(self, input_ids, output_router_logits=False, **kwargs):
                # æ¨¡æ‹Ÿæ¨¡å‹è¾“å‡º
                batch_size, seq_len = input_ids.shape
                vocab_size = 50257
                
                # æ¨¡æ‹Ÿlogits
                logits = torch.randn(batch_size, seq_len, vocab_size)
                
                # æ¨¡æ‹Ÿrouter_logitsï¼ˆå¦‚æœè¯·æ±‚ï¼‰
                if output_router_logits:
                    router_logits = [torch.randn(batch_size, seq_len, 8)]  # 8ä¸ªä¸“å®¶
                    return type('obj', (object,), {
                        'logits': logits,
                        'router_logits': router_logits
                    })()
                else:
                    return type('obj', (object,), {'logits': logits})()
        
        tokenizer = MockTokenizer()
        model = MockModel()
        model_id = "mock_model"
        print("âœ“ æ¨¡æ‹Ÿæ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # 2. åˆ›å»ºæ¨¡å‹åŒ…è£…å™¨
    print("\n2. åˆå§‹åŒ–æ¨¡å‹åŒ…è£…å™¨...")
    model_wrapper = MoEModelWrapper(model, tokenizer)
    
    # 3. è®¾ç½®æ°´å°å‚æ•°
    secret_key = "epw_enhanced_secret_key_2024"
    gamma = 0.25
    delta = 2.0
    max_new_tokens = 150
    
    # 4. è·å–è·¯ç”±å™¨å“ˆå¸Œ
    print("\n3. è®¡ç®—è·¯ç”±å™¨å“ˆå¸Œ...")
    if model_id == "mock_model":
        router_hash = "mock_router_hash_for_testing"
        print(f"âœ“ æ¨¡æ‹Ÿè·¯ç”±å™¨å“ˆå¸Œ: {router_hash}")
    else:
        try:
            router_hash = get_router_hash(model, "block_sparse_moe")
            print(f"âœ“ è·¯ç”±å™¨å“ˆå¸Œ: {router_hash[:16]}...")
        except Exception as e:
            print(f"âœ— è·¯ç”±å™¨å“ˆå¸Œè®¡ç®—å¤±è´¥: {e}")
            router_hash = "default_router_hash"
            print("ä½¿ç”¨é»˜è®¤è·¯ç”±å™¨å“ˆå¸Œ")
    
    # 5. åˆ›å»ºæ°´å°ç”Ÿæˆå™¨å’Œæ£€æµ‹å™¨
    print("\n4. åˆå§‹åŒ–æ°´å°ç»„ä»¶...")
    watermarker = EPW_A_Watermarker(
        model_wrapper=model_wrapper,
        secret_key=secret_key,
        gamma=gamma,
        delta_config={"delta": delta}
    )
    
    detector = EPW_A_Detector(
        model_wrapper=model_wrapper,
        secret_key=secret_key,
        router_hash=router_hash,
        gamma=gamma
    )
    
    # 6. ç”Ÿæˆå¸¦æ°´å°çš„æ–‡æœ¬
    print("\n5. ç”Ÿæˆå¸¦æ°´å°çš„æ–‡æœ¬...")
    prompt = "In a world where AI is becoming increasingly powerful, the ability to trace the origin of generated content is"
    
    print(f"è¾“å…¥æç¤º: {prompt}")
    
    # æ‰‹åŠ¨ç”Ÿæˆå¸¦æ°´å°çš„æ–‡æœ¬
    input_ids = tokenizer(prompt, return_tensors="pt", add_special_tokens=False).input_ids
    if hasattr(model, 'device'):
        input_ids = input_ids.to(model.device)
    generated_ids = input_ids.clone()
    
    print("æ­£åœ¨ç”Ÿæˆæ–‡æœ¬...")
    for i in range(max_new_tokens):
        # è·å–å½“å‰logits
        with torch.no_grad():
            outputs = model(generated_ids, output_router_logits=True)
        
        logits = outputs.logits[:, -1, :]
        
        # åº”ç”¨æ°´å°
        watermarked_logits = watermarker(generated_ids, logits)
        
        # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
        probs = torch.softmax(watermarked_logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        
        # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
        generated_ids = torch.cat([generated_ids, next_token], dim=1)
        
        if i % 50 == 0:
            print(f"å·²ç”Ÿæˆ {i+1}/{max_new_tokens} ä¸ªtoken...")
    
    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
    watermarked_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    
    print("\n--- ç”Ÿæˆçš„å¸¦æ°´å°æ–‡æœ¬ ---")
    print(watermarked_text)
    
    # 7. æ£€æµ‹æ°´å°
    print("\n6. æ£€æµ‹æ°´å°...")
    
    # ç°ç›’æ£€æµ‹ (CSPV)
    print("\n--- ç°ç›’æ£€æµ‹ç»“æœ (CSPV) ---")
    cspv_score = detector.detect_graybox_cspv(watermarked_text, sample_size=50)
    print(f"CSPV Z-score: {cspv_score:.4f}")
    
    # é»‘ç›’æ£€æµ‹ (PEPI) - éœ€è¦å…ˆè®­ç»ƒé¢„è¨€æœº
    print("\n--- é»‘ç›’æ£€æµ‹ç»“æœ (PEPI) ---")
    try:
        # å°è¯•åŠ è½½é¢„è®­ç»ƒçš„é¢„è¨€æœº
        oracle = load_pepi_oracle("pepi_oracle.pkl")
        print("âœ“ åŠ è½½é¢„è®­ç»ƒçš„PEPIé¢„è¨€æœº")
    except FileNotFoundError:
        print("æœªæ‰¾åˆ°é¢„è®­ç»ƒçš„PEPIé¢„è¨€æœºï¼Œè·³è¿‡é»‘ç›’æ£€æµ‹")
        print("æç¤ºï¼šå¯ä»¥ä½¿ç”¨ train_pepi_oracle() å‡½æ•°è®­ç»ƒé¢„è¨€æœº")
        oracle = None
    
    if oracle is not None:
        pepi_score = detector.detect_blackbox_pepi(watermarked_text, oracle)
        print(f"PEPI Z-score: {pepi_score:.4f}")
    
    # 8. ç»“æœåˆ†æ
    print("\n--- æ£€æµ‹ç»“æœåˆ†æ ---")
    token_ids = tokenizer(watermarked_text, return_tensors="pt", add_special_tokens=False).input_ids
    print(f"åˆ†æçš„è¯å…ƒæ•°: {token_ids.shape[1] - 1}")
    
    if model_id == "mock_model":
        print("ğŸ“ æ³¨æ„ï¼šè¿™æ˜¯æ¨¡æ‹Ÿæ¨¡å¼çš„ç»“æœï¼Œä»…ç”¨äºåŠŸèƒ½æµ‹è¯•")
        print("   åœ¨å®é™…æ¨¡å‹ä¸Šè¿è¡Œæ—¶ä¼šå¾—åˆ°æ›´å‡†ç¡®çš„ç»“æœ")
    
    if cspv_score > 4.0:
        print("âœ“ ç°ç›’æ£€æµ‹ï¼šæ£€æµ‹åˆ°é«˜ç½®ä¿¡åº¦çš„æ°´å°ä¿¡å·")
    else:
        print("âœ— ç°ç›’æ£€æµ‹ï¼šæœªæ£€æµ‹åˆ°æ˜æ˜¾çš„æ°´å°ä¿¡å·")
    
    if oracle is not None and pepi_score > 4.0:
        print("âœ“ é»‘ç›’æ£€æµ‹ï¼šæ£€æµ‹åˆ°é«˜ç½®ä¿¡åº¦çš„æ°´å°ä¿¡å·")
    elif oracle is not None:
        print("âœ— é»‘ç›’æ£€æµ‹ï¼šæœªæ£€æµ‹åˆ°æ˜æ˜¾çš„æ°´å°ä¿¡å·")
    
    print("\n=== å®éªŒå®Œæˆ ===")

# --- ç®€åŒ–æµ‹è¯•å‡½æ•° ---

def test_basic_functionality():
    """
    æµ‹è¯•åŸºæœ¬åŠŸèƒ½ï¼Œä¸ä¾èµ–å¤§å‹æ¨¡å‹
    """
    print("=== åŸºæœ¬åŠŸèƒ½æµ‹è¯• ===")
    
    # æµ‹è¯•ç»¿åå•ç”Ÿæˆ
    print("\n1. æµ‹è¯•ç»¿åå•ç”Ÿæˆ...")
    test_key = "test_secret_key"
    test_expert_index = 5
    test_router_hash = "test_router_hash_12345"
    test_vocab_size = 1000
    test_gamma = 0.3
    
    green_list = get_green_list_ids(
        test_key, test_expert_index, test_router_hash, test_vocab_size, test_gamma
    )
    print(f"âœ“ ç»¿åå•ç”ŸæˆæˆåŠŸï¼Œå¤§å°: {len(green_list)}")
    print(f"  é¢„æœŸå¤§å°: {int(test_vocab_size * test_gamma)}")
    
    # æµ‹è¯•è·¯ç”±å™¨å“ˆå¸Œè®¡ç®—
    print("\n2. æµ‹è¯•è·¯ç”±å™¨å“ˆå¸Œè®¡ç®—...")
    try:
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¨¡å‹
        class TestModel:
            def __init__(self):
                self.model = type('obj', (object,), {
                    'block_sparse_moe': type('obj', (object,), {
                        'gate': type('obj', (object,), {
                            'weight': type('obj', (object,), {
                                'data': torch.randn(10, 10)
                            })
                        })
                    })
                })()
        
        test_model = TestModel()
        test_hash = get_router_hash(test_model, "block_sparse_moe")
        print(f"âœ“ è·¯ç”±å™¨å“ˆå¸Œè®¡ç®—æˆåŠŸ: {test_hash[:16]}...")
    except Exception as e:
        print(f"âœ— è·¯ç”±å™¨å“ˆå¸Œè®¡ç®—å¤±è´¥: {e}")
    
    # æµ‹è¯•Z-scoreè®¡ç®—
    print("\n3. æµ‹è¯•Z-scoreè®¡ç®—...")
    detector = type('obj', (object,), {
        'gamma': 0.3,
        '_calculate_z_score': lambda self, green_tokens, total_tokens: (
            (green_tokens - total_tokens * self.gamma) / 
            (np.sqrt(total_tokens * self.gamma * (1 - self.gamma)) + 1e-8)
        )
    })()
    
    test_green_tokens = 35
    test_total_tokens = 100
    z_score = detector._calculate_z_score(test_green_tokens, test_total_tokens)
    print(f"âœ“ Z-scoreè®¡ç®—æˆåŠŸ: {z_score:.4f}")
    
    print("\n=== åŸºæœ¬åŠŸèƒ½æµ‹è¯•å®Œæˆ ===")

if __name__ == "__main__":
    # é¦–å…ˆè¿è¡ŒåŸºæœ¬åŠŸèƒ½æµ‹è¯•
    test_basic_functionality()
    
    # ç„¶åè¿è¡Œå®Œæ•´å®éªŒï¼ˆå¦‚æœæ¨¡å‹å¯ç”¨ï¼‰
    print("\n" + "="*50)
    print("å¼€å§‹å®Œæ•´å®éªŒ...")
    
    # åŸæœ‰çš„å®Œæ•´å®éªŒä»£ç ...


