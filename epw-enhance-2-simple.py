#!/usr/bin/env python3
"""
EPW-A å¢å¼ºç‰ˆç®€åŒ–æµ‹è¯•è„šæœ¬
ä¸“æ³¨äºåŸºæœ¬åŠŸèƒ½æµ‹è¯•ï¼Œä¸ä¾èµ–å¤§å‹æ¨¡å‹
"""

import torch
import hashlib
import random
import numpy as np
from typing import List, Tuple, Dict, Any

# --- æ ¸å¿ƒå·¥å…·å‡½æ•° ---

def get_green_list_ids(
    key: str,
    expert_index: int,
    router_hash: str,
    vocab_size: int,
    gamma: float = 0.5
) -> List[int]:
    """
    æ ¹æ®å¯†é’¥ã€ä¸“å®¶ç´¢å¼•å’Œè·¯ç”±å™¨å“ˆå¸Œç”Ÿæˆç»¿åå• (IRSHå®ç°)ã€‚
    """
    combined_input = f"{key}-{expert_index}-{router_hash}"
    hasher = hashlib.sha256()
    hasher.update(combined_input.encode('utf-8'))
    seed = int.from_bytes(hasher.digest(), 'big')
    
    rng = random.Random(seed)
    green_list_size = int(vocab_size * gamma)
    green_list = rng.sample(range(vocab_size), green_list_size)
    return green_list

def get_router_hash(model: torch.nn.Module, moe_layer_name: str = "block_sparse_moe") -> str:
    """
    è®¡ç®—å¹¶è¿”å›æ¨¡å‹è·¯ç”±å™¨æƒé‡çš„SHA256å“ˆå¸Œå€¼ (IRSHå®ç°)ã€‚
    """
    try:
        # è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹è·¯å¾„ï¼Œå®é™…è·¯å¾„å–å†³äºå…·ä½“æ¨¡å‹æ¶æ„
        router_weights = getattr(model.model, moe_layer_name).gate.weight.data
        hasher = hashlib.sha256()
        hasher.update(router_weights.cpu().numpy().tobytes())
        return hasher.hexdigest()
    except AttributeError:
        raise AttributeError(f"æ— æ³•åœ¨æ¨¡å‹ä¸­æ‰¾åˆ°åä¸º '{moe_layer_name}' çš„MoEå±‚æˆ–å…¶gateã€‚è¯·æ£€æŸ¥æ¨¡å‹æ¶æ„ã€‚")

# --- æ¨¡æ‹Ÿæ¨¡å‹åŒ…è£…å™¨ ---

class MockTokenizer:
    def __init__(self):
        self.vocab_size = 50257  # GPT-2è¯æ±‡è¡¨å¤§å°
        self.pad_token = "<|endoftext|>"
        self.eos_token = "<|endoftext|>"
    
    def encode(self, text, return_tensors='pt', add_special_tokens=False):
        # ç®€å•çš„æ¨¡æ‹Ÿç¼–ç 
        tokens = [hash(text) % self.vocab_size] + [i % self.vocab_size for i in range(len(text.split()))]
        return torch.tensor([tokens])
    
    def decode(self, token_ids, skip_special_tokens=True):
        # ç¡®ä¿token_idsæ˜¯å¼ é‡æ ¼å¼
        if isinstance(token_ids, torch.Tensor):
            if token_ids.dim() == 1:
                token_list = token_ids.tolist()
            else:
                token_list = token_ids[0].tolist()
        else:
            token_list = token_ids
        return "æ¨¡æ‹Ÿç”Ÿæˆçš„æ–‡æœ¬: " + " ".join([f"token_{i}" for i in token_list])

class MockModel:
    def __init__(self):
        self.config = type('obj', (object,), {'vocab_size': 50257})()
        self.device = 'cpu'
    
    def __call__(self, input_ids, output_router_logits=False, **kwargs):
        # æ¨¡æ‹Ÿæ¨¡å‹è¾“å‡º
        batch_size, seq_len = input_ids.shape
        vocab_size = 50257
        
        # æ¨¡æ‹Ÿlogits
        logits = torch.randn(batch_size, seq_len, vocab_size)
        
        # æ¨¡æ‹Ÿrouter_logitsï¼ˆå¦‚æœè¯·æ±‚ï¼‰
        if output_router_logits:
            router_logits = [torch.randn(batch_size, seq_len, 8)]  # 8ä¸ªä¸“å®¶
            return type('obj', (object,), {
                'logits': logits,
                'router_logits': router_logits
            })()
        else:
            return type('obj', (object,), {'logits': logits})()

class MockModelWrapper:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def get_vocab_size(self) -> int:
        return self.tokenizer.vocab_size

    def get_logits_and_route_info(self, input_ids: torch.Tensor) -> Tuple:
        """
        æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œè¿”å›logitsã€top-1ä¸“å®¶ç´¢å¼•å’Œå…¶ç½®ä¿¡åº¦ã€‚
        """
        with torch.no_grad():
            outputs = self.model(input_ids, output_router_logits=True)
            
        logits = outputs.logits[:, -1, :]
        
        # æ¨¡æ‹Ÿä¸“å®¶é€‰æ‹©
        top_expert_index = torch.randint(0, 8, (1,)).item()
        top_expert_confidence = torch.rand(1).item()
        
        return logits, top_expert_index, top_expert_confidence

    def get_logits_blackbox(self, input_ids: torch.Tensor) -> torch.Tensor:
        """
        æ¨¡æ‹Ÿé»‘ç›’APIï¼Œåªè¿”å›logitsã€‚
        """
        with torch.no_grad():
            outputs = self.model(input_ids)
        return outputs.logits[:, -1, :]

# --- æ°´å°ç”Ÿæˆå™¨ ---

class MockWatermarker:
    def __init__(self, model_wrapper, secret_key: str, gamma: float = 0.5, delta: float = 2.0):
        self.wrapper = model_wrapper
        self.secret_key = secret_key
        self.gamma = gamma
        self.delta = delta
        self.vocab_size = self.wrapper.get_vocab_size()
        self.router_hash = "mock_router_hash_for_testing"

    def __call__(self, input_ids: torch.Tensor, scores: torch.Tensor) -> torch.Tensor:
        # è·å–è·¯ç”±ä¿¡æ¯
        _, top_expert_index, confidence = self.wrapper.get_logits_and_route_info(input_ids)

        # ç”Ÿæˆç»¿åå•
        green_list = get_green_list_ids(
            self.secret_key,
            top_expert_index,
            self.router_hash,
            self.vocab_size,
            self.gamma
        )
        
        # ä¿®æ”¹ Logits
        scores[:, green_list] += self.delta
        return scores

# --- æ°´å°æ£€æµ‹å™¨ ---

class MockDetector:
    def __init__(self, model_wrapper, secret_key: str, router_hash: str, gamma: float = 0.5):
        self.wrapper = model_wrapper
        self.secret_key = secret_key
        self.router_hash = router_hash
        self.gamma = gamma
        self.vocab_size = self.wrapper.get_vocab_size()

    def _calculate_z_score(self, green_tokens: int, total_tokens: int) -> float:
        if total_tokens == 0:
            return 0.0
        expected_green = total_tokens * self.gamma
        std_dev = np.sqrt(total_tokens * self.gamma * (1 - self.gamma))
        return (green_tokens - expected_green) / (std_dev + 1e-8)

    def detect_graybox_cspv(self, text: str, sample_size: int = 50) -> float:
        """
        ä½¿ç”¨ç½®ä¿¡åº¦åˆ†å±‚è·¯å¾„éªŒè¯ (CSPV) è¿›è¡Œé«˜æ•ˆç°ç›’æ£€æµ‹ã€‚
        """
        token_ids = self.wrapper.tokenizer.encode(text, return_tensors='pt')
        if token_ids.shape[1] <= 1: 
            return 0.0

        # ç®€åŒ–çš„æ£€æµ‹é€»è¾‘
        green_token_count = 0
        num_tokens = min(sample_size, token_ids.shape[1] - 1)
        
        for t in range(1, num_tokens + 1):
            context = token_ids[:, :t]
            _, top_expert_index, _ = self.wrapper.get_logits_and_route_info(context)
            
            green_list = get_green_list_ids(
                self.secret_key, top_expert_index, self.router_hash, self.vocab_size, self.gamma
            )
            
            if token_ids[0, t].item() in green_list:
                green_token_count += 1
        
        return self._calculate_z_score(green_token_count, num_tokens)

# --- æµ‹è¯•å‡½æ•° ---

def test_basic_functionality():
    """
    æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    """
    print("=== EPW-A å¢å¼ºç‰ˆåŸºæœ¬åŠŸèƒ½æµ‹è¯• ===")
    
    # æµ‹è¯•ç»¿åå•ç”Ÿæˆ
    print("\n1. æµ‹è¯•ç»¿åå•ç”Ÿæˆ...")
    test_key = "test_secret_key"
    test_expert_index = 5
    test_router_hash = "test_router_hash_12345"
    test_vocab_size = 1000
    test_gamma = 0.3
    
    green_list = get_green_list_ids(
        test_key, test_expert_index, test_router_hash, test_vocab_size, test_gamma
    )
    print(f"âœ“ ç»¿åå•ç”ŸæˆæˆåŠŸï¼Œå¤§å°: {len(green_list)}")
    print(f"  é¢„æœŸå¤§å°: {int(test_vocab_size * test_gamma)}")
    print(f"  ç»¿åå•ç¤ºä¾‹: {green_list[:10]}...")
    
    # æµ‹è¯•ç¡®å®šæ€§
    green_list2 = get_green_list_ids(
        test_key, test_expert_index, test_router_hash, test_vocab_size, test_gamma
    )
    if green_list == green_list2:
        print("âœ“ ç»¿åå•ç”Ÿæˆå…·æœ‰ç¡®å®šæ€§")
    else:
        print("âœ— ç»¿åå•ç”Ÿæˆä¸å…·æœ‰ç¡®å®šæ€§")
    
    # æµ‹è¯•è·¯ç”±å™¨å“ˆå¸Œè®¡ç®—
    print("\n2. æµ‹è¯•è·¯ç”±å™¨å“ˆå¸Œè®¡ç®—...")
    try:
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¨¡å‹
        class TestModel:
            def __init__(self):
                self.model = type('obj', (object,), {
                    'block_sparse_moe': type('obj', (object,), {
                        'gate': type('obj', (object,), {
                            'weight': type('obj', (object,), {
                                'data': torch.randn(10, 10)
                            })
                        })
                    })
                })()
        
        test_model = TestModel()
        test_hash = get_router_hash(test_model, "block_sparse_moe")
        print(f"âœ“ è·¯ç”±å™¨å“ˆå¸Œè®¡ç®—æˆåŠŸ: {test_hash[:16]}...")
    except Exception as e:
        print(f"âœ— è·¯ç”±å™¨å“ˆå¸Œè®¡ç®—å¤±è´¥: {e}")
    
    # æµ‹è¯•Z-scoreè®¡ç®—
    print("\n3. æµ‹è¯•Z-scoreè®¡ç®—...")
    def calculate_z_score(green_tokens: int, total_tokens: int, gamma: float = 0.3) -> float:
        if total_tokens == 0:
            return 0.0
        expected_green = total_tokens * gamma
        std_dev = np.sqrt(total_tokens * gamma * (1 - gamma))
        return (green_tokens - expected_green) / (std_dev + 1e-8)
    
    test_green_tokens = 35
    test_total_tokens = 100
    z_score = calculate_z_score(test_green_tokens, test_total_tokens)
    print(f"âœ“ Z-scoreè®¡ç®—æˆåŠŸ: {z_score:.4f}")
    
    # æµ‹è¯•ä¸åŒå‚æ•°ä¸‹çš„Z-score
    test_cases = [
        (30, 100, 0.3),  # æ­£å¸¸æƒ…å†µ
        (50, 100, 0.3),  # é«˜å‘½ä¸­ç‡
        (10, 100, 0.3),  # ä½å‘½ä¸­ç‡
    ]
    
    for green_tokens, total_tokens, gamma in test_cases:
        z_score = calculate_z_score(green_tokens, total_tokens, gamma)
        print(f"  ç»¿åå•å‘½ä¸­: {green_tokens}/{total_tokens}, Z-score: {z_score:.4f}")
    
    print("\n=== åŸºæœ¬åŠŸèƒ½æµ‹è¯•å®Œæˆ ===")
    print("æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")

def test_watermark_generation():
    """
    æµ‹è¯•æ°´å°ç”ŸæˆåŠŸèƒ½
    """
    print("\n=== æ°´å°ç”Ÿæˆæµ‹è¯• ===")
    
    # åˆ›å»ºæ¨¡æ‹Ÿç»„ä»¶
    tokenizer = MockTokenizer()
    model = MockModel()
    model_wrapper = MockModelWrapper(model, tokenizer)
    
    # åˆ›å»ºæ°´å°ç”Ÿæˆå™¨
    secret_key = "test_secret_key_2024"
    gamma = 0.25
    delta = 2.0
    
    watermarker = MockWatermarker(model_wrapper, secret_key, gamma, delta)
    
    # æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
    prompt = "In a world where AI is becoming increasingly powerful"
    print(f"è¾“å…¥æç¤º: {prompt}")
    
    input_ids = tokenizer.encode(prompt, return_tensors='pt', add_special_tokens=False)
    generated_ids = input_ids.clone()
    
    print("æ­£åœ¨ç”Ÿæˆå¸¦æ°´å°çš„æ–‡æœ¬...")
    max_new_tokens = 20  # å‡å°‘tokenæ•°é‡ä»¥åŠ å¿«æµ‹è¯•
    
    for i in range(max_new_tokens):
        # è·å–å½“å‰logits
        with torch.no_grad():
            outputs = model(generated_ids, output_router_logits=True)
        
        logits = outputs.logits[:, -1, :]
        
        # åº”ç”¨æ°´å°
        watermarked_logits = watermarker(generated_ids, logits)
        
        # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
        probs = torch.softmax(watermarked_logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        
        # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
        generated_ids = torch.cat([generated_ids, next_token], dim=1)
    
    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
    watermarked_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    print(f"ç”Ÿæˆçš„æ–‡æœ¬: {watermarked_text}")
    
    # æµ‹è¯•æ°´å°æ£€æµ‹
    print("\n=== æ°´å°æ£€æµ‹æµ‹è¯• ===")
    detector = MockDetector(model_wrapper, secret_key, "mock_router_hash_for_testing", gamma)
    
    cspv_score = detector.detect_graybox_cspv(watermarked_text, sample_size=10)
    print(f"CSPV Z-score: {cspv_score:.4f}")
    
    if cspv_score > 2.0:
        print("âœ“ æ£€æµ‹åˆ°æ°´å°ä¿¡å·")
    else:
        print("âœ— æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„æ°´å°ä¿¡å·")
    
    print("\n=== æ°´å°æµ‹è¯•å®Œæˆ ===")

if __name__ == "__main__":
    # è¿è¡ŒåŸºæœ¬åŠŸèƒ½æµ‹è¯•
    test_basic_functionality()
    
    # è¿è¡Œæ°´å°ç”Ÿæˆå’Œæ£€æµ‹æµ‹è¯•
    test_watermark_generation()
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("è¿™ä¸ªç®€åŒ–ç‰ˆæœ¬å±•ç¤ºäº†EPW-Aå¢å¼ºç‰ˆçš„æ ¸å¿ƒåŠŸèƒ½ã€‚")
    print("åœ¨å®é™…ä½¿ç”¨æ—¶ï¼Œè¯·ä½¿ç”¨å®Œæ•´çš„epw-enhance-2.pyè„šæœ¬ã€‚") 